---
title: "abel_keya_week_14_IP_Part_1_and_Part 2"
output: html_notebook
---


abel_keya_week14_IP_Part 1 PCA Dimensionality Reduction using  and _Part 2  Dimensionality Reduction using tSNE
author: Abel Keya
date: "uly 24, 2020



```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

#R PROGRAMMING BASICS: EXPLOLATORY DATA ANALYSIS
##1. Defining the Question
To perform explororatory data analysis on a dataset using R programming language.
a) Specifying the Question
-Load a dataset
-preprocess the data
-find missing values
-find duplicates
-find outliers
-clean data
-perform Univariate Graphical Exploratory Data Analysis
-perform Bivariate and Multivariate Graphical Data Analysis.

b)The metric for success 
-clean dataset
-perform Univariate Graphical Exploratory Data Analysis
-perform Bivariate and Multivariate Graphical Data Analysis.
c)The context
A Kenyan entrepreneur has created an online cryptography course and would want to advertise it on her blog. She currently targets audiences originating from various countries. In the past, she ran ads to advertise a related course on the same blog and collected data in the process. She would now like to employ your services as a Data Science Consultant to help her identify which individuals are most likely to click on her ads.
d)experimental design taken 
Find and deal with outliers, anomalies, and missing data within the dataset.
Perform  univariate and bivariate analysis.
From your insights provide a conclusion and recommendation.
e) Data Relevance
The provided data was appropriate for the classification analysis that was needed.

#2. Reading the Data
```{r}
# Install the following packages:
install.packages("foreign")
library(foreign)
install.packages("car")
install.packages("Hmisc")
install.packages("reshape")
```


```{r include=FALSE}
library(dplyr)
```

```{r}
Supermarket <- read.csv(file = 'Supermarket_Dataset_1.csv')
```
3. Checking the Data
```{r}
# Previewing the dataset
# ---
# 
head(Supermarket)

```

```{r}
head(Supermarket, n=10)# First 10 rows of dataset
```

```{r}
head(Supermarket, n= -10) # All rows but the last 10
```

```{r}
tail(Supermarket) # Last 6 rows
```

```{r}
tail(Supermarket, n=10) # Last 10 rows
```

```{r}
tail(Supermarket, n= -10) # All rows but the first 10
```

```{r}
Supermarket[1:10, ] # First 10 rows
```

```{r}
Supermarket[1:10,1:3] # First 10 rows of data of the first 3 variables
```

```{r}
summary(Supermarket) # Provides basic descriptive statistics and frequencies.

```

```{r}
edit(Supermarket) # Open data editor
```

```{r}
str(Supermarket) # Provides the structure of the dataset
```

```{r}
names(Supermarket) # Lists variables in the dataset
```
#4. Tidying the Dataset


```{r}
colSums(is.na(Supermarket)) # Number of missing per column/variable
```
### FINDING DUPLICATES
```{r}
# check dimensions
dim(Supermarket)
```

```{r}
#check unique values
Supermarket <- unique(Supermarket)
Supermarket
```

```{r}
#check duplicates by rows
#duplicated_rows <- duplicated(Supermarket)
#duplicated_rows
```

```{r}
#Extract duplicate elements:
#Supermarket[duplicated(Supermarket)]
```


###FIXING MISSING DATA
```{r}
# The function complete.cases() returns a logical vector indicating which cases are complete.
# list rows of data that have missing values
Supermarket1<-Supermarket[!complete.cases(Supermarket),]
Supermarket1
```



```{r}
#confirming the dataset
Supermarket
```

```{r}
#confirm data types per column
str(Supermarket)
```


```{r}
# Type cast the column to date
Supermarket$Date<- as.Date(Supermarket$Date)
```

```{r}
#confirm the date conversion
str(Supermarket)
```

#FEATURE ENGINEERING:FORMATTING TIME/DATE COLUMNS
```{r}
install.packages("lubridate")
install.packages("tidyr")
library(tidyr)
library(lubridate)
Supermarket_2 <- separate(Supermarket, Date, c("Year", "Month", "Day"))
head(Supermarket_2)
```

```{r}
head(Supermarket_2)
```
#DESCRIPTIVE STATISTICS FOR THE DATASET

```{r}
names(Supermarket_2)
 #"Invoice.ID","Branch","Customer.type","Gender","Product.line","Unit.price","Quantity","Tax","Year","Month","Day","Time","Payment","cogs","gross.margin.percentage","gross.income","Rating","Total"
```



```{r}
#installing packages for descriptive stats
if(!require(psych)){install.packages("psych")}
if(!require(FSA)){install.packages("FSA")}
if(!require(plyr)){install.packages("plyr")}
if(!require(boot)){install.packages("boot")}
if(!require(DescTools)){install.packages("DescTools")}
```


```{r}
#DESCRIPTIVE STATISTICS FOR THE DATASET
names(Supermarket_2)
#installing packages for descriptive stats
if(!require(psych)){install.packages("psych")}
if(!require(FSA)){install.packages("FSA")}
if(!require(plyr)){install.packages("plyr")}
if(!require(boot)){install.packages("boot")}
if(!require(DescTools)){install.packages("DescTools")}
```


```{r}
# summary of descriptive statistics
library(psych)

describe(Supermarket_2)
```

```{r}
#statistiscal measures of despersion 
library(FSA)
Summarize(Quantity ~ Gender + Branch,
          data=Supermarket_2)
```

#
```{r}
# package will summarize all variables in a data frame, listing the frequencies for levels of nominal variables; and for interval/ratio data, the minimum, 1st quartile, median, mean, 3rd quartile, and maximum
summary(Supermarket_2)

```
# Skewness and kurtosis among other statistics
```{r}
#descriptive statistics for Quantity
describe(Supermarket_2$Quantity,
         type=3)         ### Type of calculation for skewness and kurtosis
```


```{r}
#"Invoice.ID","Branch","Customer.type","Gender","Product.line","Unit.price","Quantity","Tax","Year","Month","Day","Time","Payment","cogs","gross.margin.percentage","gross.income","Rating","Total"
#descriptive statistics for Unit.price
describe(Supermarket_2$Unit.price,
         type=3)
```

```{r}
#descriptive statistics for Quantity
describe(Supermarket_2$Quantity,
         type=3)
```

```{r}
#"Invoice.ID","Branch","Customer.type","Gender","Product.line","Unit.price","Quantity","Tax","Year","Month","Day","Time","Payment","cogs","gross.margin.percentage","gross.income","Rating","Total"
#descriptive statistics for gross.income
describe(Supermarket_2$gross.income,
         type=3)
```
```{r}
names(Supermarket_2)
```

#LABEL ENCODING


```{r}
colnames<-names(Supermarket_2[1:5])
colnames
```

```{r}
install.packages("superml")
```

```{r}
library(superml)
```



```{r}
#"Customer.type" "Gender"       "Product.line" 
#https://stackoverflow.com/questions/38620424/label-encoder-functionality-in-r
lbl <- LabelEncoder$new()
lbl$fit(Supermarket_2$Invoice.ID)
Supermarket_2$Invoice.ID <- lbl$fit_transform(Supermarket_2$Invoice.ID)
lbl$fit(Supermarket_2$Branch)
Supermarket_2$Branch <- lbl$fit_transform(Supermarket_2$Branch)
lbl$fit(Supermarket_2$Customer.type)
Supermarket_2$Customer.type <- lbl$fit_transform(Supermarket_2$Customer.type)
lbl$fit(Supermarket_2$Gender)
Supermarket_2$Gender<- lbl$fit_transform(Supermarket_2$Gender)
lbl$fit(Supermarket_2$Product.line)
Supermarket_2$Product.line <- lbl$fit_transform(Supermarket_2$Product.line)
lbl$fit(Supermarket_2$Payment)
Supermarket_2$Payment <- lbl$fit_transform(Supermarket_2$Payment)
```


```{r}
head(Supermarket_2)
```


```{r}
#decode_names <- lbl$inverse_transform(Supermarket_2$Invoice.ID)
```



#UNIVARIATE VISUALIZATIONS ANALYSIS
1.BARPLOTS
```{r}
barplot(table(Supermarket_2$Quantity))
```

```{r}
barplot(table(Supermarket_2$Branch))
```
2.BOXPLOTS
```{r}
boxplot.stats(Supermarket_2$Quantity)
```

```{r}
plot(x = Supermarket_2$Branch, y = Supermarket_2$Quantity)
     
```
COUNTPLOT
```{r}
counts <- table(Supermarket_2$Product.line)
  barplot(counts, main="Frequency of Product.line",  xlab="Number of Product.line")
```

```{r}
counts <- table(Supermarket_2$Customer.type)
  barplot(counts, main="Frequency of Customer.type",  xlab="Number of Customer.type")

```

```{r}
#"Unit.price","Quantity","Tax","Year","Month","Day","Time","Payment","cogs","gross.margin.percentage","gross.income","Rating","Total"
pairs(~Supermarket_2$Quantity+Supermarket_2$Tax+Supermarket_2$gross.margin.percentage+Supermarket_2$gross.income+Supermarket_2$Rating+Supermarket_2$Total, data=Supermarket_2, main="scatterplot matrix  for the Sales")
```

```{r}
barplot(table(Supermarket_2$gross.income), horiz=TRUE, main="Barplot")
boxplot(rt(100, 5), main="Boxplot")
stripchart(sample(1:20, 10, replace=TRUE), method="stack", main="Stripchart")
pie(table(sample(1:6, 10, replace=TRUE)), main="Piechart")
```

```{r}
screen(1)
barplot(Supermarket_2$gross.income, main="Barplot")
screen(2)
boxplot(sample(1:20, 100, replace=TRUE) ~ gl(4, 25, labels=LETTERS[1:4]),
        col=rainbow(4), notch=TRUE, main="Boxplot")
screen(3)
plot(sample(1:20, 40, replace=TRUE), pch=20, xlab=NA, ylab=NA,
     main="Scatter plot")
close.screen(all.screens=TRUE)
```

#BIVARIATE PLOTS

```{r}
library("ggplot2")
 #
geom_line()
ggplot(data =Supermarket_2,aes(x=gross.income,y=Quantity))+
  geom_line()

```

```{r}
install.packages("corrplot")
```
#Compute correlation matrix
```{r}
install.packages("Hmisc")
```

```{r}
Supermarket_2<-Supermarket_2[c("Invoice.ID","Branch","Customer.type","Gender","Product.line", "Unit.price","Quantity","Tax","Payment","cogs","gross.margin.percentage","gross.income","Rating","Total")]
#correlations
res <- cor(Supermarket_2)
round(res, 2)
```
##CORRELATIONS



```{r}
if(!require(corrplot)){install.packages("corrplot")}
library("corrplot")
```
#Compute correlation matrix
```{r}
# function rcorr() [in Hmisc package] is used to compute the significance levels for pearson and spearman correlations. 
if(!require(Hmisc)){install.packages("Hmisc")}
library("Hmisc")
```

```{r}
Supermarket_2<-Supermarket_2[c("Branch","Customer.type","Gender","Product.line", "Unit.price","Quantity","Tax","Payment","cogs","gross.income","Rating")]
res2 <- rcorr(as.matrix(Supermarket_2))
res2
```
Correlation matrix with significance levels (p-value)
```{r}
# Extract the correlation coefficients
res2$r
# Extract p-values
res2$P

```

```{r}
# flattenCorrMatrix
# cormat : matrix of the correlation coefficients
# pmat : matrix of the correlation p-values
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}
```

```{r}
res2<-rcorr(as.matrix(Supermarket_2))
flattenCorrMatrix(res2$r, res2$P)
```

```{r}
#Visualize correlation matrix
symnum(res, abbr.colnames = FALSE)
```

```{r}
install.packages("corrplot")
```

The function chart.Correlation()[ in the package PerformanceAnalytics],is used to display a chart of a correlation matrix.
```{r}
install.packages("PerformanceAnalytics")
```

```{r}
library("PerformanceAnalytics")
my_data <- Supermarket_2
chart.Correlation(my_data, histogram=TRUE, pch=19)
```


#ggcorrplot: Visualization of a correlation matrix using ggplot2
```{r}
#gcorrplot can be installed from CRAN as follow:
install.packages("ggcorrplot")
```

```{r}
library(ggcorrplot)
```

```{r}

# Compute a correlation matrix
data(Supermarket_2)
corr <- round(cor(Supermarket_2),1)
corr
```

```{r}
# Compute a matrix of correlation p-values
p.mat <- cor_pmat(Supermarket_2)
head(p.mat[, 1:3])

```
#Correlation matrix visualization
```{r}
# Visualize the correlation matrix
# --------------------------------
# method = "square" (default)
ggcorrplot(corr)
```



```{r}
# Add correlation coefficients
# --------------------------------
# argument lab = TRUE
ggcorrplot(corr, hc.order = TRUE, type = "lower",
   lab = TRUE)
```

#CATEGORICAL VARIABLE ENCODING
```{r include=FALSE}
#library('fastDummies')
install.packages("dummies")
```

```{r}
Supermarket_2.new<-Supermarket[c("Branch","Customer.type","Gender","Product.line", "Unit.price","Quantity","Tax","Payment","cogs","gross.margin.percentage","gross.income","Rating","Total")]
library(dummies)
Supermarket_2.new <- dummy.data.frame(Supermarket_2[], sep = ".")
Supermarket_2.new
```


```{r}
head(Supermarket_2.new)
```

```{r}
names(Supermarket_2.new)
```

```{r}
#Removing the missing data

Supermarket_2.new <- na.omit(Supermarket_2.new) 
```

```{r}
#select unique values
Supermarket_2.new <- unique(Supermarket_2.new)
Supermarket_2.new
```

```{r}
#check duplicates by rows
duplicated_rows <- duplicated(Supermarket_2.new)
duplicated_rows
```


#Principal Component Analysis
```{r}
Supermarket_2.new<- as.data.frame(scale(Supermarket_2.new))
Supermarket_2.pca <- prcomp(Supermarket_2.new)
```
#principal component analysis using the “prcomp()” function in R
```{r}
# summary of the principal component analysis results using the “summary()” function on the output of “prcomp()”
#The standard deviation of the components is stored in a named element called “sdev” of the output variable made by “prcomp”
summary(Supermarket_2.pca)

```

```{r}
Supermarket_2.pca$sdev
```

```{r}
#The total variance explained by the components is the sum of the variances of the components:
sum(Supermarket_2.pca$sdev)^2
```
 
```{r}
#summarise the results of a principal components analysis by making a scree plot
screeplot(Supermarket_2.pca, type="lines")
```
The change in slope in the scree plot occurs at component, which is the “elbow” of the scree plot. Therefore, based on the scree plot that the first components should before the elbow be retained.

Kaiser’s criterion: only retain principal components for which the variance is above 1 (when principal component analysis was applied to standardised data). I  check this by finding the variance of each of the principal components:
```{r}
(Supermarket_2.pca$sdev)^2
```
principal components to retain is by minimum amount of the total variance.
```{r}
Supermarket_2.pca$rotation[,1]
```
Loadings for the Principal Components
The loadings for the principal components are stored in a named element “rotation” of the variable returned by “prcomp()”. This contains a matrix with the loadings of each principal component, where the first column in the matrix contains the loadings for the first principal component, the second column contains the loadings for the second principal component.
```{r}
sum(Supermarket_2.pca$rotation[,1])^2
```

Scatterplots of the Principal Components
The values of the principal components are stored in a named element “x” of the variable returned by “prcomp()”. 
```{r}
plot(Supermarket_2.pca$x[,1],Supermarket_2.pca$x[,2]) # make a scatterplot
text(Supermarket_2.pca$x[,1],Supermarket_2.pca$x[,2], Supermarket_2$Product.line,cex=0.7, pos=4, col="red") # add labels
#The scatterplot shows the first principal component on the x-axis, and the second principal component on the y-axis. 
```

Part 2:Feature Selection using tSNE

```{r}
# Installing Rtnse package
install.packages("Rtsne")
```



```{r}
# Loading our tnse library
library(Rtsne)
```

```{r}
# Curating the database for analysis 
Labels<-Supermarket_2$Branch
Supermarket_2$Branch<-as.factor(Supermarket_2$Branch)
```

```{r}
# For plotting
colors = rainbow(length(unique(Supermarket_2$Branch)))
names(colors) = unique(Supermarket_2$Branch)
```

```{r}
# Executing the algorithm on curated data
tsne <- Rtsne(Supermarket_2[,-1], dims = 3, perplexity=30, verbose=TRUE, max_iter = 500)
```

```{r}
#Getting the duration of execution
exeTimeTsne <- system.time(Rtsne(Supermarket_2[,-1], dims = 2, perplexity=30, verbose=TRUE, max_iter = 500))
```

```{r}
# Plotting our graph 
# 
plot(tsne$Y, t='n', main="tsne")
text(tsne$Y, labels=Supermarket_2$Branch, col=colors[Supermarket_2$Branch])
```
#Based on prouduct line using ggplot2
```{r}
metadata <- data.frame(sample_id = rownames(Supermarket_2),
                       colour = Supermarket_2$Product.line)
data <- as.matrix(Supermarket_2[,1:10])
```

```{r}
df <- data.frame(x = tsne$Y[,1],
                 y = tsne$Y[,2],
                 colour = metadata$colour)
```

```{r}
install.packages("ggplot2")
```
```{r}
library(ggplot2)
```


```{r}
ggplot(df, aes(x, y, colour = colour)) +
  geom_point()
```

Part 2: Feauture Selection

```{r}
# Installing and loading our caret package
# ---
# 
suppressWarnings(
        suppressMessages(if
                         (!require(caret, quietly=TRUE))
                install.packages("caret")))
library(caret)
```

```{r}
# Installing and loading the corrplot package for plotting

suppressWarnings(
        suppressMessages(if
                         (!require(corrplot, quietly=TRUE))
                install.packages("corrplot")))
library(corrplot)

```


```{r}
# Calculating the correlation matrix
# ---
#
correlationMatrix <- cor(Supermarket_2.new)

# Find attributes that are highly correlated
# ---
#
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)

# Highly correlated attributes
# ---
# 
highlyCorrelated

names(Supermarket_2.new[,highlyCorrelated])
```

```{r}
#remove the variables with a higher correlation 
# and comparing the results graphically 

# Removing Redundant Features 

Supermarket_3.new<-Supermarket_2.new[-highlyCorrelated]
```


```{r}
Supermarket_3.new
```

```{r}
# Performing our graphical comparison
# ---
# 
par(mfrow = c(1, 2))
corrplot(correlationMatrix, order = "hclust")
corrplot(cor(Supermarket_3.new), order = "hclust")
```

```{r}

```
Feature selection by wrapper methods

```{r}
# Installing and loading our clustvarsel package

suppressWarnings(
        suppressMessages(if
                         (!require(clustvarsel, quietly=TRUE))
                install.packages("clustvarsel")))
                         
library(clustvarsel)
```

```{r}
# Installing and loading our mclust package

suppressWarnings(
        suppressMessages(if
                         (!require(mclust, quietly=TRUE))
                install.packages("mclust")))
library(mclust)
```

```{r}
# Sequential forward greedy search (default)
# ---
#
out = clustvarsel(Supermarket_2.new, G = 1:15)
out
```

```{r}
# The selection algorithm would indicate that the subset 
# used for the clustering model is composed of variables X1 and X2 
# and that other variables should be rejected. 
# Having identified the variables to be  used, proceed to build the clustering model:
Subset1 = Supermarket_2.new[,out$subset]
mod = Mclust(Subset1, G = 1:15)
summary(mod)
```

```{r}
plot(mod,c("classification"))
```



#Feature Selection by Embedded Methods
```{r}

# use the ewkm function from the wskm package.
# This is a weighted subspace clustering algorithm that is well suited to very high dimensional data.
```

```{r}
#install and load the wskm package

suppressWarnings(
        suppressMessages(if
                         (!require(wskm, quietly=TRUE))
                install.packages("wskm")))
```



```{r}
library(wskm)

set.seed(2)
model <- ewkm(Supermarket_2.new, 3, lambda=2, maxiter=1000)
```

```{r}
# Loading and installing our cluster package
suppressWarnings(
        suppressMessages(if
                         (!require(cluster, quietly=TRUE))
                install.packages("cluster")))
library("cluster")
# Cluster Plot against 1st 2 principal components
clusplot(Supermarket_2.new, model$cluster, color=TRUE, shade=TRUE,
         labels=2, lines=1,main='Cluster Analysis for Supermarket')
```

```{r}
# Weights are calculated for each variable and cluster. 
# They are a measure of the relative importance of each variable are incorporated into the distance function, 
# typically reducing the distance for more important variables check them 
round(model$weights*100,2)
```

# Feature Ranking
```{r}
#use the FSelector Package. This is a package containing functions for selecting attributes from a given dataset.
suppressWarnings(
        suppressMessages(if
                         (!require(FSelector, quietly=TRUE))
                install.packages("FSelector")))
library(FSelector)
```

```{r}
# From the FSelector package, use the correlation coefficient as a unit of valuation. 
# This would be one of the several algorithms contained 
# in the FSelector package that can be used rank the variables.

Scores <- linear.correlation(Branch~., Supermarket_2.new)
Scores
```


```{r}
#define a cutoff use the top 5 representative variables, 
# through the use of the cutoff.k function included in the FSelector package. 
# cutoff.k: The algorithms select a subset from a ranked attributes. 
Subset <- cutoff.k(Scores, 5)
as.data.frame(Subset)
```

```{r}
#set cutoff as a percentage which would indicate to work with the percentage of the best variables.
Subset2 <-cutoff.k.percent(Scores, 0.4)
as.data.frame(Subset2)
```

```{r}
#use an entropy - based approach as shown below;
Scores2 <- information.gain(Branch~., Supermarket_2.new)
# Choosing Variables by cutoffSubset <- cutoff.k(Scores2, 5)
Subset3 <- cutoff.k(Scores2, 5)
as.data.frame(Subset3)
```





